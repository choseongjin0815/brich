from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import StaleElementReferenceException
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
from datetime import datetime
from datetime import datetime, timedelta
from fake_useragent import UserAgent

import json
import requests
import re
import time 
import math
import sys

# # Chrome ë“œë¼ì´ë²„ ì˜µì…˜ ì„¤ì •
options = Options()
options.add_argument("--headless")  # í™”ë©´ ì—†ì´ ì‹¤í–‰
options.add_argument("--disable-gpu")
options.add_argument("--no-sandbox")
ua = UserAgent()

# random user-agent ì„¤ì •
def get_pc_user_agent():
    while True:
        candidate = ua.random
        if not re.search(r'Mobile|Android|iPhone', candidate, re.I):
            return candidate
user_agent = get_pc_user_agent()
options.add_argument(f'user-agent={user_agent}')



# service = Service("/path/to/chromedriver")  # chromedriver ê²½ë¡œ
driver = webdriver.Chrome(options)

def is_toplist_open(driver):
    try:
        wrapper = driver.find_element(By.ID, "toplistWrapper")

        # display, height, aria ìƒíƒœ ê°ì§€
        display = driver.execute_script("return window.getComputedStyle(arguments[0]).display;", wrapper)
        height = driver.execute_script("return arguments[0].offsetHeight;", wrapper)
        aria_hidden = wrapper.get_attribute("aria-hidden") or wrapper.get_attribute("area-hidden")

        visible = wrapper.is_displayed()
        aria_open = (aria_hidden is None) or (aria_hidden.lower() == "false")

        # ğŸ”§ ì™„í™”ëœ ì¡°ê±´: display != none ë˜ëŠ” height > 50
        return (display != "none" or height > 50) and aria_open and visible

    except Exception:
        return False


def ensure_toplist_open(driver, timeout=10):
    try:
        # ë¨¼ì € span í…ìŠ¤íŠ¸ë¡œ ìƒíƒœ íŒë‹¨
        span_text = driver.find_element(By.ID, "toplistSpanBlind").text.strip()
        print(f"í˜„ì¬ ëª©ë¡ ìƒíƒœ í…ìŠ¤íŠ¸: {span_text}")

        # ì´ë¯¸ ì—´ë ¤ ìˆëŠ” ê²½ìš°
        if "ëª©ë¡ë‹«ê¸°" in span_text:
            print("ëª©ë¡ì´ ì´ë¯¸ ì—´ë ¤ ìˆìŒ â†’ í´ë¦­ ìƒëµ")
            return
        time.sleep(3)  # ì•ˆì • ëŒ€ê¸°
        print("ëª©ë¡ì´ ë‹«í˜€ ìˆìŒ â†’ ëª©ë¡ ì—´ê¸° í´ë¦­")
        toggle_btn = WebDriverWait(driver, timeout).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "a._toggleTopList"))
        )

        driver.execute_script("arguments[0].scrollIntoView(true);", toggle_btn)
        driver.execute_script("arguments[0].click();", toggle_btn)

        # ì—´ë¦´ ë•Œê¹Œì§€ ëŒ€ê¸° (span í…ìŠ¤íŠ¸ê°€ 'ëª©ë¡ë‹«ê¸°'ë¡œ ë°”ë€ŒëŠ”ì§€ í™•ì¸)
        WebDriverWait(driver, timeout).until(
            lambda d: "ëª©ë¡ë‹«ê¸°" in d.find_element(By.ID, "toplistSpanBlind").text
        )

        print("ëª©ë¡ ì—´ë¦¼ í™•ì¸ ì™„ë£Œ")

    except TimeoutException:
        print("âš ï¸ Timeout: ëª©ë¡ì´ ì—´ë¦¬ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì´ë¯¸ ì—´ë ¤ ìˆê±°ë‚˜ ëª©ë¡ ì—†ìŒì¼ ìˆ˜ ìˆìŒ)")

# ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì§„ì…
blg_url = sys.argv[1]
driver.get(blg_url) # ë¸”ë¡œê·¸ IDë¥¼ ì¸ìë¡œ ë°›ìŒ
wait = WebDriverWait(driver, 5)

# 1. iframeì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° í›„ ì „í™˜
wait.until(EC.frame_to_be_available_and_switch_to_it((By.ID, "mainFrame")))

blog_link = WebDriverWait(driver, 3).until(
    EC.element_to_be_clickable((By.XPATH, "//a[contains(@href, 'PostList.naver') and contains(@class, 'itemfont') and contains(@class, '_doNclick') and contains(@class, '_param(false|blog|)')]"))
    ,print("ë¸”ë¡œê·¸ íƒ­ í´ë¦­")
)
blog_link.click()

# âœ… ì „ì²´ë³´ê¸° í´ë¦­
try:
    all_posts_link = WebDriverWait(driver, 5).until(
        EC.element_to_be_clickable((By.XPATH, "//a[@id='category0' and contains(text(), 'ì „ì²´ë³´ê¸°')]"))
    )
    driver.execute_script("arguments[0].click();", all_posts_link)
    print("ì „ì²´ë³´ê¸° í´ë¦­ ì™„ë£Œ")
except TimeoutException:
    print("ì „ì²´ë³´ê¸° ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")




# 1ï¸âƒ£ ëª©ë¡ ì—´ë¦¼ ìƒíƒœ ë³´ì¥
ensure_toplist_open(driver)

# 2ï¸âƒ£ ì ê¹ ëŒ€ê¸° (ëª©ë¡ DOM ì™„ì „íˆ ê°±ì‹ ë  ë•Œê¹Œì§€)
time.sleep(0.5)

# 3ï¸âƒ£ ëª©ë¡ì´ ì—´ë¦° ìƒíƒœì˜ HTMLë¡œ ìƒˆë¡œ íŒŒì‹±
soup = BeautifulSoup(driver.page_source, 'html.parser')

# 4ï¸âƒ£ í˜ì´ì§€ ê°œìˆ˜ ì¶”ì¶œ
page_count_elem = soup.select_one('h4.category_title.pcol2')
numeric_chars = [char for char in page_count_elem.text if char.isdigit()]
numeric_string = "".join(numeric_chars)


# list_size = soup.select_one('#listCountView').text
# list_size = re.findall(r'\d+', list_size)[0]
links = set()  # âœ… setìœ¼ë¡œ ì¤‘ë³µ ë°©ì§€
total_pages = math.ceil(int(numeric_string) / 5)

for page_num in range(1, 5):
    # âœ… í˜„ì¬ í˜ì´ì§€ HTML ìƒˆë¡œ íŒŒì‹±
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    # âœ… ë§í¬ ìˆ˜ì§‘ (ì ˆëŒ€ê²½ë¡œ + ì •í™•í•œ í´ë˜ìŠ¤ í•„í„°)
    for a in soup.find_all('a', href=True):
        href = a['href']
        classes = a.get('class', [])
        if (
            href.startswith('https://blog.naver.com/PostView.naver?blogId=') and
            all(c in classes for c in ['pcol2', '_setTop', '_setTopListUrl']) and
            not a.has_attr('logno') and
            not a.has_attr('onclick')
        ):
            links.add(href)  # setì´ë¼ ì¤‘ë³µ ì•ˆ ë¨

    print(f"[PAGE {page_num}] ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: {len(links)}")

    # âœ… ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ í´ë¦­
    next_xpath = f"//a[contains(@class,'_goPageTop') and contains(@class,'_param({page_num+1})')]"
    try:
        next_button = WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.XPATH, next_xpath))
        )
        driver.execute_script("arguments[0].click();", next_button)
        
        # âœ… í˜ì´ì§€ê°€ ì‹¤ì œë¡œ ë°”ë€” ë•Œê¹Œì§€ ëŒ€ê¸°
        WebDriverWait(driver, 10).until(EC.staleness_of(next_button))
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.blog2_list"))
        )
    except TimeoutException:
        print(f"í˜ì´ì§€ {page_num}ì—ì„œ ë‹¤ìŒ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ë§ˆì§€ë§‰ í˜ì´ì§€ì¼ ìˆ˜ ìˆìŒ).")
        break

print(f"ì´ ê³ ìœ  ë§í¬ ìˆ˜: {len(links)}")
for l in sorted(links):
    print(l)
   


# í´ë¦­ í›„ì˜ HTML ê°€ì ¸ì˜¤ê¸°
html = driver.page_source
print(html[:1000])  # ì•ë¶€ë¶„ë§Œ ì¶œë ¥í•´ë³´ê¸°

count = 1
results = []
for idx, post_url in enumerate(links):
    driver.get(post_url)
    WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, 'iframe')))
    try:
        iframe = driver.find_element(By.ID, "mainFrame")
        driver.switch_to.frame(iframe)
    except Exception:
        pass
    
    # í”„ë ˆì„ ì „í™˜ í›„
    WebDriverWait(driver, 10).until(lambda d: d.execute_script("return document.readyState") == "complete")

    soup = BeautifulSoup(driver.page_source, 'html.parser')

    like_elem = soup.select_one('span.u_likeit_text._count.num')
    # ê³µê° ìˆ˜ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ë‹¤ì‹œ ì‹œë„
    if not like_elem or not like_elem.text.strip():
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        like_elem = soup.select_one('span.u_likeit_text._count.num')

    like_count = like_elem.text.strip() if like_elem and like_elem.text.strip() else 'N/A'

    # ë‚ ì§œ
    post_elem = soup.select_one('span.se_publishDate.pcol2')
    post_date = post_elem.text.strip() if post_elem else 'N/A'

    now = datetime.now()
    if re.search(r'(ì‹œê°„|ë¶„|ì¼)\s*ì „', post_date):
        post_datetime = now
    else:
        try:
            post_datetime = datetime.strptime(post_date, "%Y. %m. %d. %H:%M")
            if now - post_datetime < timedelta(days=7):
                print(f"ìµœê·¼ 7ì¼ ì´ë‚´ í¬ìŠ¤íŠ¸ â†’ ì œì™¸: {post_date}")
                continue  # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ì§€ ì•Šê³  ë‹¤ìŒ í¬ìŠ¤íŠ¸ë¡œ ë„˜ì–´ê°
        except ValueError:
            try:
                post_datetime = datetime.strptime(post_date, "%Y. %m. %d.")
            except ValueError:
                print(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {post_date}")
                continue

    # post_date_str = post_datetime.strftime("%Y-%m-%d")

    # ëŒ“ê¸€
    comment_elem = soup.select_one('em._commentCount')
    comment_count = comment_elem.text.strip() if comment_elem else 'N/A'

    results.append({
        'index': idx,
        'url': post_url,
        'date': post_date,
        'likes': like_count,
        'comments': comment_count
    })

    print(f"{post_url} | ë‚ ì§œ: {post_date} | ê³µê°: {like_count} | ëŒ“ê¸€: {comment_count} | ì¸ë±ìŠ¤: {idx+1}")

# ====== ë‚ ì§œ ì •ë ¬ (ë¬¸ìì—´ â†’ datetime ë³€í™˜ í›„ ìµœì‹ ìˆœ ì •ë ¬) ======
def parse_date_safe(s: str) -> datetime:
    s = s.replace(".", "-").replace(" ", "").strip()
    try:
        return datetime.strptime(s, "%Y-%m-%d")
    except ValueError:
        # "YYYY-MM-DDHH:MM" ê°™ì´ ë¶™ì€ ê²½ìš°
        try:
            return datetime.strptime(s[:10], "%Y-%m-%d")
        except Exception:
            return datetime.min

results.sort(key=lambda x: parse_date_safe(x["date"]), reverse=True)
url = "http://localhost:8080/api/results"

def safe_int(value):
    try:
        return int(value)
    except:
        return 0
data = {
    "blgAddrs": blg_url,   
    "postList": [           
        {
            "pstUrl": r["url"],
            "pstCmnt": safe_int(r["comments"]),
            "pstLk": safe_int(r["likes"]),
            "pstdDt": r["date"]
        }
        for r in results
    ]
}
print("=== ì „ì†¡ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ===")
print(json.dumps(data, indent=2, ensure_ascii=False))
print("==========================")

response = requests.post(
    url,
    headers={"Content-Type": "application/json"},
    json=data  
)
driver.quit()

