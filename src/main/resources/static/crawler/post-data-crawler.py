from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import StaleElementReferenceException
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
from datetime import datetime
from datetime import datetime, timedelta
from fake_useragent import UserAgent

import os
import json
import requests
import re
import time 
import math
import sys

# # Chrome ë“œë¼ì´ë²„ ì˜µì…˜ ì„¤ì •
options = Options()
options.add_argument("--headless")  # í™”ë©´ ì—†ì´ ì‹¤í–‰
options.add_argument("--disable-gpu")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
ua = UserAgent()

# random user-agent ì„¤ì •
def get_pc_user_agent():
    while True:
        candidate = ua.random
        if not re.search(r'Mobile|Android|iPhone', candidate, re.I):
            return candidate
user_agent = get_pc_user_agent()
options.add_argument(f'user-agent={user_agent}')



# service = Service("/path/to/chromedriver")  # chromedriver ê²½ë¡œ
driver = webdriver.Chrome()

def is_toplist_open(driver):
    try:
        wrapper = driver.find_element(By.ID, "toplistWrapper")

        # display, height, aria ìƒíƒœ ê°ì§€
        display = driver.execute_script("return window.getComputedStyle(arguments[0]).display;", wrapper)
        height = driver.execute_script("return arguments[0].offsetHeight;", wrapper)
        aria_hidden = wrapper.get_attribute("aria-hidden") or wrapper.get_attribute("area-hidden")

        visible = wrapper.is_displayed()
        aria_open = (aria_hidden is None) or (aria_hidden.lower() == "false")

        # ğŸ”§ ì™„í™”ëœ ì¡°ê±´: display != none ë˜ëŠ” height > 50
        return (display != "none" or height > 50) and aria_open and visible

    except Exception:
        return False


def ensure_toplist_open(driver, timeout=5):
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ì˜ Toplist(ëª©ë¡)ê°€ ë‹«í˜€ ìˆì„ ê²½ìš° ì—´ë¦¬ë„ë¡ ë³´ì¥í•œë‹¤.
    """
    try:
        span_elem = WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.ID, "toplistSpanBlind"))
        )
        span_text = span_elem.text.strip()
        print(f"í˜„ì¬ ëª©ë¡ ìƒíƒœ í…ìŠ¤íŠ¸: {span_text}")

        # ì´ë¯¸ ì—´ë¦° ê²½ìš°
        if "ëª©ë¡ë‹«ê¸°" in span_text:
            print("ëª©ë¡ì´ ì´ë¯¸ ì—´ë ¤ ìˆìŒ â†’ í´ë¦­ ìƒëµ")
            return

        print("ëª©ë¡ì´ ë‹«í˜€ ìˆìŒ â†’ ëª©ë¡ ì—´ê¸° ì‹œë„")

        # í† ê¸€ ë²„íŠ¼ ì°¾ê¸°
        toggle_btn = WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "a._toggleTopList"))
        )

        # ë‚´ë¶€ JS ì´ë²¤íŠ¸ë¡œ ê°•ì œ íŠ¸ë¦¬ê±° (ê¸°ë³¸ click()ì€ ë™ì‘ ì•ˆ í•¨)
        driver.execute_script("""
            const el = arguments[0];
            const evt = document.createEvent('MouseEvents');
            evt.initEvent('click', true, true);
            el.dispatchEvent(evt);
        """, toggle_btn)

        # ëª©ë¡ ì—´ë¦´ ë•Œê¹Œì§€ ì¬ì‹œë„í•˜ë©° ê¸°ë‹¤ë¦¬ê¸°
        WebDriverWait(driver, timeout * 2, ignored_exceptions=[StaleElementReferenceException]).until(
            lambda d: "ëª©ë¡ë‹«ê¸°" in d.find_element(By.ID, "toplistSpanBlind").text
        )

        print("âœ… ëª©ë¡ ì—´ë¦¼ í™•ì¸ ì™„ë£Œ")

    except TimeoutException:
        print("âš ï¸ Timeout: ëª©ë¡ì´ ì—´ë¦¬ì§€ ì•ŠìŒ (ì´ë¯¸ ì—´ë ¤ ìˆê±°ë‚˜ ì—†ëŠ” ê²½ìš°)")
    except StaleElementReferenceException:
        print("âš ï¸ StaleElementReference ë°œìƒ â†’ ì¬ì‹œë„ ì¤‘...")
        time.sleep(0.5)
        return ensure_toplist_open(driver, timeout)


def load_last_url():
    if os.path.exists("latest_post.json"):
        with open("latest_post.json", "r", encoding="utf-8") as f:
            return json.load(f).get("latest_post_url", None)
    return None

def save_last_url(url):
    with open("latest_post.json", "w", encoding="utf-8") as f:
        json.dump({"latest_post_url": url}, f, ensure_ascii=False)



# ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì§„ì…
blg_url = sys.argv[1]
driver.get(blg_url) # ë¸”ë¡œê·¸ IDë¥¼ ì¸ìë¡œ ë°›ìŒ
wait = WebDriverWait(driver, 30)

# 1. iframeì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° í›„ ì „í™˜
wait.until(EC.frame_to_be_available_and_switch_to_it((By.ID, "mainFrame")))

blog_link = WebDriverWait(driver, 3).until(
    EC.element_to_be_clickable((By.XPATH, "//a[contains(@href, 'PostList.naver') and contains(@class, 'itemfont') and contains(@class, '_doNclick') and contains(@class, '_param(false|blog|)')]"))
    ,print("ë¸”ë¡œê·¸ íƒ­ í´ë¦­")
)
blog_link.click()

# âœ… ì „ì²´ë³´ê¸° í´ë¦­
try:
    all_posts_link = WebDriverWait(driver, 5).until(
        EC.element_to_be_clickable((By.XPATH, "//a[@id='category0' and contains(text(), 'ì „ì²´ë³´ê¸°')]"))
    )
    driver.execute_script("arguments[0].click();", all_posts_link)
    print("ì „ì²´ë³´ê¸° í´ë¦­ ì™„ë£Œ")
except TimeoutException:
    print("ì „ì²´ë³´ê¸° ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")




# 1ï¸âƒ£ ëª©ë¡ ì—´ë¦¼ ìƒíƒœ ë³´ì¥
ensure_toplist_open(driver)

# 2ï¸âƒ£ ì ê¹ ëŒ€ê¸° (ëª©ë¡ DOM ì™„ì „íˆ ê°±ì‹ ë  ë•Œê¹Œì§€)
time.sleep(0.5)

# 3ï¸âƒ£ ëª©ë¡ì´ ì—´ë¦° ìƒíƒœì˜ HTMLë¡œ ìƒˆë¡œ íŒŒì‹±
soup = BeautifulSoup(driver.page_source, 'html.parser')

# 4ï¸âƒ£ í˜ì´ì§€ ê°œìˆ˜ ì¶”ì¶œ
page_count_elem = soup.select_one('h4.category_title.pcol2')
numeric_chars = [char for char in page_count_elem.text if char.isdigit()]
numeric_string = "".join(numeric_chars)


# list_size = soup.select_one('#listCountView').text
# list_size = re.findall(r'\d+', list_size)[0]
last_url = load_last_url()
stop_collecting = False
links = []  # âœ… setìœ¼ë¡œ ì¤‘ë³µ ë°©ì§€
seen = set()
total_pages = math.ceil(int(numeric_string) / 5)



for page_num in range(1, 50):
    # âœ… í˜„ì¬ í˜ì´ì§€ HTML ìƒˆë¡œ íŒŒì‹±
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    # âœ… ë§í¬ ìˆ˜ì§‘ (ì ˆëŒ€ê²½ë¡œ + ì •í™•í•œ í´ë˜ìŠ¤ í•„í„°)
    for a in soup.find_all('a', href=True):
        href = a['href']
        classes = a.get('class', [])
        if (
            href.startswith('https://blog.naver.com/PostView.naver?blogId=') and
            all(c in classes for c in ['pcol2', '_setTop', '_setTopListUrl']) and
            not a.has_attr('logno') and
            not a.has_attr('onclick') and
            href not in seen
        ):
            if last_url and href == last_url:
                print(f"âœ… ë§ˆì§€ë§‰ ìˆ˜ì§‘ í¬ìŠ¤íŠ¸ ë„ë‹¬: {href} â†’ í¬ë¡¤ë§ ì¤‘ë‹¨")
                stop_collecting = True
                break
            
            links.append(href)  
            seen.add(href) # setì´ë¼ ì¤‘ë³µ ì•ˆ ë¨

    if stop_collecting:
        break
    print(f"[PAGE {page_num}] ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: {len(links)}")

    # âœ… ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ í´ë¦­
    next_xpath = f"//a[contains(@class,'_goPageTop') and contains(@class,'_param({page_num+1})')]"
    try:
        next_button = WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.XPATH, next_xpath))
        )
        driver.execute_script("arguments[0].click();", next_button)
        
        # âœ… í˜ì´ì§€ê°€ ì‹¤ì œë¡œ ë°”ë€” ë•Œê¹Œì§€ ëŒ€ê¸°
        WebDriverWait(driver, 10).until(EC.staleness_of(next_button))
        WebDriverWait(driver, 2).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.blog2_list"))
        )
    except TimeoutException:
        print(f"í˜ì´ì§€ {page_num}ì—ì„œ ë‹¤ìŒ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ë§ˆì§€ë§‰ í˜ì´ì§€ì¼ ìˆ˜ ìˆìŒ).")
        break




print(f"ì´ ê³ ìœ  ë§í¬ ìˆ˜: {len(links)}")
for l in sorted(links):
    print(l)
   

# í´ë¦­ í›„ì˜ HTML ê°€ì ¸ì˜¤ê¸°
html = driver.page_source
print(html[:1000])  # ì•ë¶€ë¶„ë§Œ ì¶œë ¥í•´ë³´ê¸°

count = 1
results = []

for idx, post_url in enumerate(links):
    driver.get(post_url)
    WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, 'iframe')))
    try:
        iframe = driver.find_element(By.ID, "mainFrame")
        driver.switch_to.frame(iframe)
    except Exception:
        pass
    
    # í”„ë ˆì„ ì „í™˜ í›„
    WebDriverWait(driver, 10).until(lambda d: d.execute_script("return document.readyState") == "complete")

    soup = BeautifulSoup(driver.page_source, 'html.parser')

    like_elem = soup.select_one('span.u_likeit_text._count.num')
    # ê³µê° ìˆ˜ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ë‹¤ì‹œ ì‹œë„
    if not like_elem or not like_elem.text.strip():
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        like_elem = soup.select_one('span.u_likeit_text._count.num')

    like_count = like_elem.text.strip() if like_elem and like_elem.text.strip() else 'N/A'

    # ë‚ ì§œ
    post_elem = soup.select_one('span.se_publishDate.pcol2')
    post_date = post_elem.text.strip() if post_elem else 'N/A'
    
    now = datetime.now()
    # ====== ë‚ ì§œ íŒŒì‹± ======
    if re.search(r'(ì‹œê°„|ë¶„|ì¼)\s*ì „', post_date):
        print(f"ìµœê·¼ 5ì¼ ì´ë‚´ í¬ìŠ¤íŠ¸ â†’ ì œì™¸: {post_date}")
        continue  # ìµœê·¼ 5ì¼ ì´ë‚´ í¬ìŠ¤íŠ¸ ì œì™¸
    else:
        try:
            post_datetime = datetime.strptime(post_date, "%Y. %m. %d. %H:%M")
            if now - post_datetime < timedelta(days=5):
                print(f"ìµœê·¼ 5ì¼ ì´ë‚´ í¬ìŠ¤íŠ¸ â†’ ì œì™¸: {post_date}")
                continue
        except ValueError:
            try:
                post_datetime = datetime.strptime(post_date, "%Y. %m. %d.")
            except ValueError:
                print(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {post_date}")
                continue

    # ====== ëŒ“ê¸€ ======
    comment_elem = soup.select_one('em._commentCount')
    comment_count = comment_elem.text.strip() if comment_elem else 'N/A'

    results.append({
        'index': idx,
        'url': post_url,
        'date': post_datetime,
        'likes': like_count,
        'comments': comment_count
    })

    print(f"{post_url} | ë‚ ì§œ: {post_datetime} | ê³µê°: {like_count} | ëŒ“ê¸€: {comment_count} | ì¸ë±ìŠ¤: {idx+1}")


# ====== ë‚ ì§œ ê¸°ì¤€ ì •ë ¬ ======
# results.sort(key=lambda x: x["date"], reverse=False)
if results:
    # ê²°ê³¼ ì¤‘ ê°€ì¥ ìµœì‹ (ë‚ ì§œê°€ ê°€ì¥ í°) í¬ìŠ¤íŠ¸ ì°¾ê¸°
    newest = max(results, key=lambda r: r["date"])
    save_last_url(newest["url"])
    print(f"ğŸ†• ìµœì‹  í¬ìŠ¤íŠ¸ URL ì €ì¥ ì™„ë£Œ â†’ {newest['url']}")
else:
    print("ìƒˆë¡œìš´ í¬ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë‘ 5ì¼ ì´ë‚´)")

url = "http://localhost:8080/api/results"

def safe_int(value):
    try:
        return int(value)
    except:
        return 0
data = {
    "blgAddrs": blg_url,   
    "postList": [           
        {
            "pstUrl": r["url"],
            "pstCmnt": safe_int(r["comments"]),
            "pstLk": safe_int(r["likes"]),
            "pstdDt": r["date"].strftime("%Y-%m-%d %H:%M")
        }
        for r in results
    ]
}
print("=== ì „ì†¡ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ===")
print(json.dumps(data, indent=2, ensure_ascii=False))
print("==========================")

response = requests.post(
    url,
    headers={"Content-Type": "application/json"},
    json=data  
)
driver.quit()
